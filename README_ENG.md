# ğŸ§  ai-helm-linter

> Microservice to audit Helm Charts using LLMs (OpenAI or Ollama), detecting errors, inconsistencies, or bad practices via specialized prompts.

---

## ğŸš€ Features

- âœ… Audits syntax, conventions, and coherence in Helm Charts  
- ğŸ“š Analyzes `Chart.yaml`, `values.yaml`, and templates under `templates/`  
- ğŸ¤– Supports Ollama local models or GPT-4o via OpenAI  
- ğŸ§© CLI and REST API via Flask  
- ğŸ³ Docker-ready and deployable with Helm + ArgoCD  
- ğŸ” Automatic fallback to OpenAI if Ollama fails  
- ğŸ“ GitOps and CI/CD friendly  
- âœï¸ Editable prompts to match your organization's policies  

---

## ğŸ“¦ Project Structure

```
ai-helm-linter/
â”œâ”€â”€ app.py                 # Flask microservice entrypoint
â”œâ”€â”€ cli/                   # CLI interface (lint.py)
â”œâ”€â”€ lib/                   # Core logic and LLM clients
â”œâ”€â”€ prompts/               # Prompt templates
â”œâ”€â”€ routes/                # Flask route handlers
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # Containerization config
â”œâ”€â”€ Makefile               # Build and deploy automation
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ§© Component Overview

### `app.py`

Flask server initializing the route `/lint-chart`, receiving JSON and invoking the linter.

### `routes/lint_chart.py`

- Endpoint: `/lint-chart`
- Input JSON:  
  ```json
  {
    "chart_path": "./charts/example",
    "mode": "ollama",
    "ruleset": "default"
  }
  ```
- Output: AI-generated lint report

### `cli/lint.py`

CLI tool to lint Helm charts from terminal.

```bash
python3 cli/lint.py --mode ollama --chart charts/example
```

### `lib/`

- `linter.py`: LLM-powered linting engine  
- `utils.py`: Loads prompts and Helm chart content  
- `ollama_client.py`: Wrapper for Ollama's API  
- `openai_client.py`: Wrapper for OpenAI's API

### `prompts/`

- `helm_linting.prompt`: Prompt template with instructions
- Easily extendable for custom org rules

---

## ğŸ” Jenkins Integration

You can invoke the microservice from a Jenkins declarative pipeline:

```groovy
def jsonPayload = '''
{
  "chart_path": "./charts/mychart",
  "mode": "ollama"
}
'''

sh '''
curl -X POST http://helm-linter.devops-ai.svc.cluster.local:5000/lint-chart   -H "Content-Type: application/json"   -d '${jsonPayload}'
'''
```

---

## ğŸ› ï¸ Getting Started (Local)

```bash
git clone https://github.com/dorado-ai-devops/ai-helm-linter.git
cd ai-helm-linter
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

### Run CLI

```bash
python3 cli/lint.py --mode ollama --chart charts/example
```

### Run Flask microservice

```bash
python3 app.py
```

---

## ğŸ’¡ Example Output

```
[ERROR] Chart.yaml is missing the "version" field.
[SUGGESTION] Add a "version: 1.0.0" key to comply with standards.
[REVIEW] The deployment.yaml template hardcodes port 80.
```

---

## ğŸ”® Roadmap

- [ ] Add security validations (runAsNonRoot, capabilities)
- [ ] Integrate kubeval or similar validators
- [ ] Output JSON for external dashboards

---

## ğŸ‘¨â€ğŸ’» Author

- **Dani** â€“ [@dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ§  Inspired by

- [Helm Best Practices](https://helm.sh/docs/chart_best_practices/)
- [Ollama](https://ollama.com)
- [OpenAI API](https://platform.openai.com/docs)

---

## ğŸ›¡ License

GNU General Public License v3.0