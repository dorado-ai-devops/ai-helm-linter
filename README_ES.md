# ğŸ§  ai-helm-linter

> Microservicio para auditar Helm Charts usando LLMs (OpenAI u Ollama), detectando errores, inconsistencias o malas prÃ¡cticas mediante prompts especializados.

---

## ğŸš€ CaracterÃ­sticas

- âœ… Audita sintaxis, convenciones y coherencia en Helm Charts  
- ğŸ“š Analiza `Chart.yaml`, `values.yaml` y las plantillas en `templates/`  
- ğŸ¤– Compatible con modelos locales de Ollama o GPT-4o vÃ­a OpenAI  
- ğŸ§© CLI y API REST vÃ­a Flask  
- ğŸ³ Preparado para Docker y desplegable con Helm + ArgoCD  
- ğŸ” Fallback automÃ¡tico a OpenAI si Ollama falla  
- ğŸ“ Compatible con GitOps y CI/CD  
- âœï¸ Prompts editables para alinearse con polÃ­ticas internas  

---

## ğŸ“¦ Estructura del Proyecto

```
ai-helm-linter/
â”œâ”€â”€ app.py                 # Entrada del microservicio Flask
â”œâ”€â”€ cli/                   # Interfaz CLI (lint.py)
â”œâ”€â”€ lib/                   # LÃ³gica central y clientes LLM
â”œâ”€â”€ prompts/               # Plantillas de prompts
â”œâ”€â”€ routes/                # Rutas de Flask
â”œâ”€â”€ requirements.txt       # Dependencias de Python
â”œâ”€â”€ Dockerfile             # ConfiguraciÃ³n de contenedor
â”œâ”€â”€ Makefile               # AutomatizaciÃ³n de build y despliegue
â””â”€â”€ README.md              # DocumentaciÃ³n del proyecto
```

---

## ğŸ§© Componentes

### `app.py`

Servidor Flask que inicia el endpoint `/lint-chart`, recibe JSON e invoca el linter.

### `routes/lint_chart.py`

- Endpoint: `/lint-chart`
- JSON de entrada:  
  ```json
  {
    "chart_path": "./charts/example",
    "mode": "ollama",
    "ruleset": "default"
  }
  ```
- Salida: Informe de lint generado por IA

### `cli/lint.py`

Herramienta CLI para ejecutar el linter desde terminal.

```bash
python3 cli/lint.py --mode ollama --chart charts/example
```

### `lib/`

- `linter.py`: NÃºcleo del anÃ¡lisis con LLM  
- `utils.py`: Carga prompts y contenido del chart  
- `ollama_client.py`: Cliente HTTP para Ollama  
- `openai_client.py`: Cliente para OpenAI

### `prompts/`

- `helm_linting.prompt`: Plantilla base de prompt
- FÃ¡cilmente extensible con reglas personalizadas

---

## ğŸ” IntegraciÃ³n Jenkins

Puedes invocar el microservicio desde un pipeline declarativo en Jenkins:

```groovy
def jsonPayload = '''
{
  "chart_path": "./charts/mychart",
  "mode": "ollama"
}
'''

sh '''
curl -X POST http://helm-linter.devops-ai.svc.cluster.local:5000/lint-chart   -H "Content-Type: application/json"   -d '${jsonPayload}'
'''
```

---

## ğŸ› ï¸ Primeros pasos (Local)

```bash
git clone https://github.com/dorado-ai-devops/ai-helm-linter.git
cd ai-helm-linter
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

### Ejecutar CLI

```bash
python3 cli/lint.py --mode ollama --chart charts/example
```

### Ejecutar microservicio Flask

```bash
python3 app.py
```

---

## ğŸ’¡ Ejemplo de salida

```
[ERROR] Chart.yaml no contiene el campo "version".
[SUGERENCIA] AÃ±ade una clave "version: 1.0.0" para cumplir con el estÃ¡ndar.
[REVISIÃ“N] La plantilla deployment.yaml fija el puerto 80 de forma rÃ­gida.
```

---

## ğŸ”® PrÃ³ximos pasos

- [ ] Validaciones de seguridad (runAsNonRoot, capabilities)
- [ ] Integrar kubeval u otros validadores YAML
- [ ] Exportar JSON estructurado para dashboards externos

---

## ğŸ‘¨â€ğŸ’» Autor

- **Dani** â€“ [@dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ§  Inspirado en

- [Buenas prÃ¡cticas Helm](https://helm.sh/docs/chart_best_practices/)
- [Ollama](https://ollama.com)
- [OpenAI API](https://platform.openai.com/docs)

---

## ğŸ›¡ Licencia

Licencia PÃºblica General GNU v3.0